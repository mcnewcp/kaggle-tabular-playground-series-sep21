{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Tabular Playground September 2021\r\n",
    "\r\n",
    "This notebook handles model tuning for the logistic regression model developed for the Kaggle Tabular Playground September 2021 Competition.  For EDA, FE, and initial model development, see the previous notebooks [here](https://github.com/mcnewcp/kaggle-tabular-playground-series-sep21/blob/coy/kaggle-tab-playground-2021-09_MI-PCA.ipynb) and [here](https://github.com/mcnewcp/kaggle-tabular-playground-series-sep21/blob/coy/kaggle-tab-playground-2021-09_FE%2BLogReg-RF-XGB.ipynb).\r\n",
    "\r\n",
    "# Load Data"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "source": [
    "import numpy as np\r\n",
    "import pandas as pd\r\n",
    "\r\n",
    "test = pd.read_csv('datasets/test.csv')\r\n",
    "train = pd.read_csv('datasets/train.csv')\r\n",
    "\r\n",
    "X_train = train.drop(['id', 'claim'], axis=1)\r\n",
    "y_train = train.claim"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Model Pipeline\r\n",
    "\r\n",
    "## Feature Engineering"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "source": [
    "from sklearn.base import BaseEstimator, TransformerMixin\r\n",
    "from sklearn.preprocessing import StandardScaler\r\n",
    "\r\n",
    "class CombinedAttributesAdder(BaseEstimator, TransformerMixin):\r\n",
    "    #one hyperparameter per new feature\r\n",
    "    def __init__(self, add_sum = True, add_num_nan = True): #no *args or **kargs\r\n",
    "        self.add_sum = add_sum\r\n",
    "        self.add_num_nan = add_num_nan\r\n",
    "    def fit(self, X, y=None):\r\n",
    "        return self #nothing to fit\r\n",
    "    def transform(self, X):\r\n",
    "        #generate additional features\r\n",
    "        if self.add_sum:\r\n",
    "            std_scaler = StandardScaler()\r\n",
    "            sum_col = X.copy()\r\n",
    "            sum_col[np.isnan(sum_col)] = 0\r\n",
    "            sum_col = std_scaler.fit_transform(sum_col)\r\n",
    "            sum_col = sum_col.sum(axis=1)\r\n",
    "            X = np.c_[X, sum_col]\r\n",
    "        if self.add_num_nan:\r\n",
    "            num_nan = np.isnan(X).sum(axis=1)\r\n",
    "            X = np.c_[X, num_nan]\r\n",
    "        return X\r\n",
    "\r\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Preprocessing"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "source": [
    "from sklearn.impute import SimpleImputer\r\n",
    "from sklearn.preprocessing import StandardScaler\r\n",
    "\r\n",
    "scaler = StandardScaler()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Model"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "source": [
    "from sklearn.pipeline import Pipeline\r\n",
    "from sklearn.ensemble import RandomForestClassifier\r\n",
    "\r\n",
    "model = Pipeline([\r\n",
    "    ('attr_adder', CombinedAttributesAdder()),\r\n",
    "    ('imputer', SimpleImputer()),\r\n",
    "    ('std_scaler', scaler),\r\n",
    "    ('rf', RandomForestClassifier(n_jobs=4))\r\n",
    "])"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Tuning\r\n",
    "\r\n",
    "For tuning I'm going to use the `Optuna` package.  Optuna provides a number of really cool features when it comes to tuning hyperparameters and processing steps, including built in plotly diagnostic plots.  A pretty good intro to Optuna can be found [here](https://towardsdatascience.com/how-to-make-your-model-awesome-with-optuna-b56d490368af). \r\n",
    "\r\n",
    "The package works by minimizing an objective function.  The objective function must be defined so that it returns a single value which is the score minimized by the optuna study.  It is suggested to use cv score when scoring inside the objective function.  All possible hyperparameters must be chosen using the built in `trial.suggest_**` functions.  5 possible distributions options are provided:\r\n",
    "\r\n",
    "* uniform — float values\r\n",
    "* log-uniform — float values\r\n",
    "* discrete uniform — float values with intervals\r\n",
    "* integer — integer values\r\n",
    "* categorical — categorical values from a list\r\n",
    "\r\n",
    "Of course you can access any hyperparameters along the pipeline using the `step__hyperparam` nomenclature.  I'm also including a pickling step in the objective below, so that I can load intermediate results in case the process is interrupted or if I want to add onto the study."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "source": [
    "import optuna\r\n",
    "import joblib\r\n",
    "from sklearn.metrics import roc_auc_score\r\n",
    "from sklearn.metrics import make_scorer\r\n",
    "from sklearn.model_selection import cross_val_score\r\n",
    "\r\n",
    "auc_scorer = make_scorer(roc_auc_score)\r\n",
    "\r\n",
    "def objective(trial):\r\n",
    "\r\n",
    "    joblib.dump(study, 'study-rf.pkl')\r\n",
    "\r\n",
    "    attr_adder__add_sum = trial.suggest_categorical('attr_adder__add_sum', [False, True])\r\n",
    "    attr_adder__add_num_nan = trial.suggest_categorical('attr_adder__add_num_nan', [False, True])\r\n",
    "    imputer__strategy = trial.suggest_categorical('imputer__strategy', ['median', 'mean', 'constant', 'most_frequent'])\r\n",
    "    rf__max_features = trial.suggest_int('rf__max_features', 1, 50)\r\n",
    "    rf__n_estimators = trial.suggest_int('rf__n_estimators', 10, 1000)\r\n",
    "\r\n",
    "    params = {\r\n",
    "        'attr_adder__add_sum': attr_adder__add_sum,\r\n",
    "        'attr_adder__add_num_nan': attr_adder__add_num_nan,\r\n",
    "        'imputer__strategy': imputer__strategy,\r\n",
    "        'rf__max_features': rf__max_features,\r\n",
    "        'rf__n_estimators': rf__n_estimators\r\n",
    "    }\r\n",
    "\r\n",
    "    model.set_params(**params)\r\n",
    "\r\n",
    "    return np.mean(cross_val_score(model, X_train, y_train, scoring = auc_scorer, n_jobs=4, cv=3))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Now all that's left to do is create the study (or load a previously pickled version) and optimize the objective.  You can specify how long the study lasts in number of trials (`n_trials`) or in time in seconds (`timeout`).  Setting `timeout` defines the time which the last trial must start before, and therefore the study last longer than the `timeout`."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "source": [
    "study = optuna.create_study(direction='maximize')\r\n",
    "study.optimize(objective, timeout=8000)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "\u001b[32m[I 2021-09-21 17:50:21,296]\u001b[0m A new study created in memory with name: no-name-b3b0c235-cd90-44b7-ada0-b114fc4ca6d4\u001b[0m\n",
      "\u001b[32m[I 2021-09-21 21:34:47,828]\u001b[0m Trial 0 finished with value: 0.7727621134927748 and parameters: {'attr_adder__add_sum': False, 'attr_adder__add_num_nan': True, 'imputer__strategy': 'median', 'rf__max_features': 18, 'rf__n_estimators': 975}. Best is trial 0 with value: 0.7727621134927748.\u001b[0m\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Visualize Results\r\n",
    "\r\n",
    "Optuna provides some really interesting visuals baked right into the study object.  Most are built with plotly, meaning you get a little interactivity to play with.\r\n",
    "\r\n",
    "## Hyperparameter Importance"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "source": [
    "import plotly\r\n",
    "optuna.visualization.plot_param_importances(study)"
   ],
   "outputs": [
    {
     "output_type": "error",
     "ename": "ValueError",
     "evalue": "Cannot evaluate parameter importances with only a single trial.",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-7-3e8c2f1da468>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mplotly\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0moptuna\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvisualization\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mplot_param_importances\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstudy\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mC:\\Python\\Miniconda\\envs\\test-env\\lib\\site-packages\\optuna\\visualization\\_param_importances.py\u001b[0m in \u001b[0;36mplot_param_importances\u001b[1;34m(study, evaluator, params, target, target_name)\u001b[0m\n\u001b[0;32m    110\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mgo\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mFigure\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlayout\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mlayout\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    111\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 112\u001b[1;33m     importances = optuna.importance.get_param_importances(\n\u001b[0m\u001b[0;32m    113\u001b[0m         \u001b[0mstudy\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mevaluator\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mevaluator\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mparams\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtarget\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    114\u001b[0m     )\n",
      "\u001b[1;32mC:\\Python\\Miniconda\\envs\\test-env\\lib\\site-packages\\optuna\\importance\\__init__.py\u001b[0m in \u001b[0;36mget_param_importances\u001b[1;34m(study, evaluator, params, target)\u001b[0m\n\u001b[0;32m     91\u001b[0m         \u001b[1;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Evaluator must be a subclass of BaseImportanceEvaluator.\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     92\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 93\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mevaluator\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstudy\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mparams\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtarget\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mC:\\Python\\Miniconda\\envs\\test-env\\lib\\site-packages\\optuna\\importance\\_fanova\\_evaluator.py\u001b[0m in \u001b[0;36mevaluate\u001b[1;34m(self, study, params, target)\u001b[0m\n\u001b[0;32m     85\u001b[0m             )\n\u001b[0;32m     86\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 87\u001b[1;33m         \u001b[0mdistributions\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_get_distributions\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstudy\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     88\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdistributions\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     89\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mOrderedDict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Python\\Miniconda\\envs\\test-env\\lib\\site-packages\\optuna\\importance\\_base.py\u001b[0m in \u001b[0;36m_get_distributions\u001b[1;34m(study, params)\u001b[0m\n\u001b[0;32m     67\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     68\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0m_get_distributions\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstudy\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mStudy\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mList\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[0mDict\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mBaseDistribution\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 69\u001b[1;33m     \u001b[0m_check_evaluate_args\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstudy\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     70\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     71\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mparams\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Python\\Miniconda\\envs\\test-env\\lib\\site-packages\\optuna\\importance\\_base.py\u001b[0m in \u001b[0;36m_check_evaluate_args\u001b[1;34m(study, params)\u001b[0m\n\u001b[0;32m    116\u001b[0m         \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Cannot evaluate parameter importances without completed trials.\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    117\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcompleted_trials\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 118\u001b[1;33m         \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Cannot evaluate parameter importances with only a single trial.\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    119\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    120\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mparams\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Cannot evaluate parameter importances with only a single trial."
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Optimization History"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "optuna.visualization.plot_optimization_history(study)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Hyperparameter Slices\r\n",
    "\r\n",
    "The slice plots give you an idea of the affect of each hyperparameter individually on the outcome of the objective."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "optuna.visualization.plot_slice(study)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Or you can look at hyperparameters individually."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "optuna.visualization.plot_slice(study, ['rf__n_estimators'])"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Evaluation\r\n",
    "\r\n",
    "Now to predict the test set with the final model and submit predictions."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "final_mod = model.set_params(**study.best_params)\r\n",
    "final_mod.fit(X_train, y_train)\r\n",
    "\r\n",
    "# Preprocessing of test data, get predictions\r\n",
    "X_test_ids = test.id\r\n",
    "X_test = test.drop('id', axis=1)\r\n",
    "preds = final_mod.predict(X_test)\r\n",
    "\r\n",
    "#export predictions\r\n",
    "output = pd.DataFrame({'id': X_test_ids,\r\n",
    "                       'claim': preds})\r\n",
    "output.to_csv('submission.csv', index=False)"
   ],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python",
   "version": "3.9.4",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.9.4 64-bit ('test-env': conda)"
  },
  "interpreter": {
   "hash": "217afb6d47e1ecfe9bc2bb1d008138758dfc335ae7a13eb273c90375d6240835"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}