{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Tabular Playground September 2021\r\n",
    "\r\n",
    "This notebook handles model tuning for the logistic regression model developed for the Kaggle Tabular Playground September 2021 Competition.  For EDA, FE, and initial model development, see the previous notebooks [here](https://github.com/mcnewcp/kaggle-tabular-playground-series-sep21/blob/coy/kaggle-tab-playground-2021-09_MI-PCA.ipynb) and [here](https://github.com/mcnewcp/kaggle-tabular-playground-series-sep21/blob/coy/kaggle-tab-playground-2021-09_FE%2BLogReg-RF-XGB.ipynb).\r\n",
    "\r\n",
    "# Load Data"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "import numpy as np\r\n",
    "import pandas as pd\r\n",
    "\r\n",
    "test = pd.read_csv('datasets/test.csv')\r\n",
    "train = pd.read_csv('datasets/train.csv')\r\n",
    "\r\n",
    "X_train = train.drop(['id', 'claim'], axis=1)\r\n",
    "y_train = train.claim\r\n",
    "\r\n",
    "features = X_train.columns.tolist()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Model Pipeline\r\n",
    "\r\n",
    "## Feature Engineering"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "from sklearn.base import BaseEstimator, TransformerMixin\r\n",
    "from sklearn.preprocessing import StandardScaler\r\n",
    "from scipy import stats\r\n",
    "\r\n",
    "class CombinedAttributesAdder(BaseEstimator, TransformerMixin):\r\n",
    "    #one hyperparameter per new feature\r\n",
    "    def __init__(\r\n",
    "        self, \r\n",
    "        add_sum = True, \r\n",
    "        add_num_nan = True,\r\n",
    "        add_abs_sum = True, \r\n",
    "        add_sem = True\r\n",
    "    ): #no *args or **kargs\r\n",
    "        self.add_sum = add_sum\r\n",
    "        self.add_num_nan = add_num_nan\r\n",
    "        self.add_abs_sum = add_abs_sum\r\n",
    "        self.add_sem = add_sem\r\n",
    "    def fit(self, X, y=None):\r\n",
    "        return self #nothing to fit\r\n",
    "    def transform(self, X):\r\n",
    "        #generate additional features\r\n",
    "        if self.add_sum:\r\n",
    "            std_scaler = StandardScaler()\r\n",
    "            sum_col = X.copy()\r\n",
    "            sum_col[np.isnan(sum_col)] = 0\r\n",
    "            sum_col = std_scaler.fit_transform(sum_col)\r\n",
    "            sum_col = sum_col.sum(axis=1)\r\n",
    "            X = np.c_[X, sum_col]\r\n",
    "        if self.add_num_nan:\r\n",
    "            num_nan = np.isnan(X).sum(axis=1)\r\n",
    "            X = np.c_[X, num_nan]\r\n",
    "        if self.add_abs_sum:\r\n",
    "            abs_sum = X.copy()\r\n",
    "            abs_sum[np.isnan(abs_sum)] = 0\r\n",
    "            abs_sum = np.abs(abs_sum).sum(axis=1)\r\n",
    "            X = np.c_[X, abs_sum]\r\n",
    "        if self.add_sem:\r\n",
    "            sem_col = stats.sem(X, nan_policy = 'omit', axis=1)\r\n",
    "            X = np.c_[X, sem_col]\r\n",
    "        return X\r\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Preprocessing"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "from sklearn.impute import SimpleImputer\r\n",
    "from sklearn.preprocessing import StandardScaler\r\n",
    "\r\n",
    "scaler = StandardScaler()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Model"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "from sklearn.pipeline import Pipeline\r\n",
    "from lightgbm import LGBMClassifier\r\n",
    "\r\n",
    "model = Pipeline([\r\n",
    "    ('attr_adder', CombinedAttributesAdder()),\r\n",
    "    ('imputer', SimpleImputer()),\r\n",
    "    ('std_scaler', scaler),\r\n",
    "    ('lgbm', LGBMClassifier(silent=False, objective='binary'))\r\n",
    "])"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Tuning\r\n",
    "\r\n",
    "For tuning I'm going to use the `Optuna` package.  Optuna provides a number of really cool features when it comes to tuning hyperparameters and processing steps, including built in plotly diagnostic plots.  A pretty good intro to Optuna can be found [here](https://towardsdatascience.com/how-to-make-your-model-awesome-with-optuna-b56d490368af). \r\n",
    "\r\n",
    "The package works by minimizing an objective function.  The objective function must be defined so that it returns a single value which is the score minimized by the optuna study.  It is suggested to use cv score when scoring inside the objective function.  All possible hyperparameters must be chosen using the built in `trial.suggest_**` functions.  5 possible distributions options are provided:\r\n",
    "\r\n",
    "* uniform — float values\r\n",
    "* log-uniform — float values\r\n",
    "* discrete uniform — float values with intervals\r\n",
    "* integer — integer values\r\n",
    "* categorical — categorical values from a list\r\n",
    "\r\n",
    "Of course you can access any hyperparameters along the pipeline using the `step__hyperparam` nomenclature.  I'm also including a pickling step in the objective below, so that I can load intermediate results in case the process is interrupted or if I want to add onto the study."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "import optuna\r\n",
    "import joblib\r\n",
    "from sklearn.metrics import roc_auc_score\r\n",
    "from sklearn.metrics import make_scorer\r\n",
    "from sklearn.model_selection import cross_val_score\r\n",
    "\r\n",
    "auc_scorer = make_scorer(roc_auc_score)\r\n",
    "\r\n",
    "def objective(trial):\r\n",
    "\r\n",
    "    joblib.dump(study, 'study-lgbm.pkl')\r\n",
    "\r\n",
    "    attr_adder__add_sum = trial.suggest_categorical('attr_adder__add_sum', [False, True])\r\n",
    "    attr_adder__add_num_nan = trial.suggest_categorical('attr_adder__add_num_nan', [False, True])\r\n",
    "    attr_adder__add_abs_sum = trial.suggest_categorical('attr_adder__add_abs_sum', [False, True])\r\n",
    "    attr_adder__add_sem = trial.suggest_categorical('attr_adder__add_sem', [False, True])\r\n",
    "    imputer__strategy = trial.suggest_categorical('imputer__strategy', ['median', 'mean', 'constant', 'most_frequent'])\r\n",
    "    lgbm__boosting_type = trial.suggest_categorical('lgbm__boosting_type', ['gbdt', 'goss'])\r\n",
    "    lgbm__num_leaves = trial.suggest_int('lgbm__num_leaves', 1, 200)\r\n",
    "    lgbm__max_depth = trial.suggest_int('lgbm__max_depth', -1, 50)\r\n",
    "    lgbm__learning_rate = trial.suggest_uniform('lgbm__learning_rate', 0, 1)\r\n",
    "    lgbm__n_estimators = trial.suggest_int('lgbm__n_estimators', 10, 500)\r\n",
    "    lgbm__reg_alpha = trial.suggest_loguniform('lgbm__reg_alpha', 0.1, 100)\r\n",
    "    lgbm__reg_lambda = trial.suggest_loguniform('lgbm__reg_lambda', 0.1, 100)\r\n",
    "\r\n",
    "\r\n",
    "    params = {\r\n",
    "        'attr_adder__add_sum': attr_adder__add_sum,\r\n",
    "        'attr_adder__add_num_nan': attr_adder__add_num_nan,\r\n",
    "        'attr_adder__add_abs_sum': attr_adder__add_abs_sum, \r\n",
    "        'attr_adder__add_sem': attr_adder__add_sem, \r\n",
    "        'imputer__strategy': imputer__strategy,\r\n",
    "        'lgbm__boosting_type': lgbm__boosting_type,\r\n",
    "        'lgbm__num_leaves': lgbm__num_leaves,\r\n",
    "        'lgbm__max_depth': lgbm__max_depth,\r\n",
    "        'lgbm__learning_rate': lgbm__learning_rate, \r\n",
    "        'lgbm__n_estimators': lgbm__n_estimators, \r\n",
    "        'lgbm__reg_alpha': lgbm__reg_alpha, \r\n",
    "        'lgbm__reg_lambda': lgbm__reg_lambda\r\n",
    "    }\r\n",
    "\r\n",
    "    model.set_params(**params)\r\n",
    "\r\n",
    "    return np.mean(cross_val_score(model, X_train, y_train, scoring = auc_scorer, n_jobs=4, cv=3))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Now all that's left to do is create the study (or load a previously pickled version) and optimize the objective.  You can specify how long the study lasts in number of trials (`n_trials`) or in time in seconds (`timeout`).  Setting `timeout` defines the time which the last trial must start before, and therefore the study last longer than the `timeout`."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "study = optuna.create_study(direction='maximize')\r\n",
    "study.optimize(objective, timeout=14400)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Visualize Results\r\n",
    "\r\n",
    "Optuna provides some really interesting visuals baked right into the study object.  Most are built with plotly, meaning you get a little interactivity to play with.\r\n",
    "\r\n",
    "## Hyperparameter Importance"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "import plotly\r\n",
    "optuna.visualization.plot_param_importances(study)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Optimization History"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "optuna.visualization.plot_optimization_history(study)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Hyperparameter Slices\r\n",
    "\r\n",
    "The slice plots give you an idea of the affect of each hyperparameter individually on the outcome of the objective."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "optuna.visualization.plot_slice(study)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Or you can look at hyperparameters individually."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "optuna.visualization.plot_slice(study, ['lgbm__learning_rate'])"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Evaluation\r\n",
    "\r\n",
    "Now to predict the test set with the final model and submit predictions."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "final_mod = model.set_params(**study.best_params)\r\n",
    "final_mod.fit(X_train, y_train)\r\n",
    "\r\n",
    "# Preprocessing of test data, get predictions\r\n",
    "X_test_ids = test.id\r\n",
    "X_test = test.drop('id', axis=1)\r\n",
    "preds = final_mod.predict(X_test)\r\n",
    "\r\n",
    "#export predictions\r\n",
    "output = pd.DataFrame({'id': X_test_ids,\r\n",
    "                       'claim': preds})\r\n",
    "output.to_csv('submission.csv', index=False)"
   ],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python",
   "version": "3.9.4",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.9.4 64-bit ('test-env': conda)"
  },
  "interpreter": {
   "hash": "217afb6d47e1ecfe9bc2bb1d008138758dfc335ae7a13eb273c90375d6240835"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}